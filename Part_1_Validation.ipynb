{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alignment of SMP and Snow Pits / Evaluation of Proksch (P15) coefficients\n",
    "*Josh King, Environment and Climate Change Canada, 2019*\n",
    "\n",
    "This workbook introduces a snow on sea ice calibration procedure for SMP-derived estimates of density first introduced in [Proksch, et al., 2015](https://agupubs.onlinelibrary.wiley.com/doi/10.1002/2014JF003266). Where indicated, the work modifies portions of the [SMP python package from SLF](https://github.com/slf-dot-ch/snowmicropyn) and uses a number of open source community packages to facilitate processing.\n",
    "\n",
    "I'm still not great at GIS in python so the maps in the publication were done in ESRI ArcMap.\n",
    "\n",
    "### ***Alignment takes a long time due to the large number of scaling candidates. If you want to skip that part and just load the result set, set `skip_alignment` below to `True`.***\n",
    "\n",
    "### Notes on settings and constants\n",
    "**CUTTER_SIZE** defines the half height in mm of the density cutter used as reference. Can be changed to accommodate different sampler sizes. No need to change this for ECCC data. \n",
    "\n",
    "**WINDOW_SIZE** defines the size of the rolling window used in SLF shot noise calculations. A 5 mm window was used in [Proksch, et al., 2015](https://agupubs.onlinelibrary.wiley.com/doi/10.1002/2014JF003266) when there was separation between the SMP and density cutter. Increasing the window reduces sensitivity to sharp transitions and reduces resolution of the analysis. However, moving to something like 2.5 mm makes comparison difficult as some of the very fine structure resolved as very different over the ~10 cm separation between the SMP and density profiles. \n",
    "\n",
    "**NUM_TESTS** defines how many random scaling configurations to test against when attempting to align the SMP and snow pit data. We brute-force the alignment in our paper so `NUM_TESTS` must be large to ensure the test space searched is sufficient. A lower number of tests risks poor alignment and therefore poor calibration. In the paper we use 10k permutations.\n",
    "\n",
    "**MAX_STRETCH_LAYER** and **MAX_STRETCH_OVERALL** define how much an individual layer can be eroded or dilated, and the maximum change in total length of the SMP profile, respectively. We allow a rather large 70% change to individual layers to accommodate pinching out but restrict the total change to 10% to avoid overfitting.\n",
    "\n",
    "**H_RESAMPLE** and **L_RESAMPLE** define the resampled resolution of the SMP and the layer size used for matching profiles. These terms are interactive with the layer stretching and should be evaluated carefully if changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Community packages\n",
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from scipy import stats\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "\n",
    "# Local packages\n",
    "import smpfunc # SMP helper functions\n",
    "\n",
    "# Import SLF SMP Package\n",
    "from snowmicropyn import Profile, proksch2015, loewe2012\n",
    "\n",
    "# Import data\n",
    "pit_summary = pd.read_csv(\"./data/Pit/pit_summary.csv\")\n",
    "pit_density = pd.read_csv(\"./data/Pit/pit_density.csv\")\n",
    "input_data = os.path.abspath(\"./data/SMP/Calibration\")\n",
    "\n",
    "# Set constants\n",
    "CUTTER_SIZE = 15  # Half the height of the density cutter in mm\n",
    "WINDOW_SIZE = 5  # SMP analysis window in mm\n",
    "H_RESAMPLE = 1  # Delta height in mm for standardized SMP profiles\n",
    "L_RESAMPLE = 50  # Layer unit height in mm for SMP matching\n",
    "MAX_STRETCH_LAYER = 0.75  # Max layer change in % of height\n",
    "MAX_STRETCH_OVERALL = 0.15  # Max profile change in % of total height\n",
    "NUM_TESTS = 10000  # Number of scaling candidates to generate for alignment testing \n",
    "\n",
    "# Set conditions\n",
    "skip_alignment = True # Set as true to just load the results from a pickle instead of reprocessing\n",
    "paper_conditions = True # Set as true to reproduce the paper results with seeding\n",
    "\n",
    "# Small differences in comparison to the paper will occur if a seed is not set.\n",
    "# This is mainly because we use a brute-force approach to matching the smp and \n",
    "# snow pit profiles with modest search size (specified by NUM_TESTS).\n",
    "if paper_conditions:\n",
    "    np.random.seed(2019) \n",
    "\n",
    "os.makedirs('./output/figures', exist_ok=True)    \n",
    "    \n",
    "def rmse(data):\n",
    "    return np.sqrt(np.mean(data**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SMP calibration profiles, should be 25 for the ECCC case\n",
    "# Preprocess the files to remove noise and detect interfaces\n",
    "def load_smp(smp_file):\n",
    "    p = Profile.load(smp_file)\n",
    "    p = smpfunc.preprocess(p, smoothing = 0, noise_threshold = 0.01)\n",
    "    ground  = p.detect_ground()\n",
    "    surface  = p.detect_surface()\n",
    "    return p\n",
    "\n",
    "file_list = []\n",
    "for file in os.listdir(input_data):\n",
    "    if file.endswith(\".pnt\"):\n",
    "        file_list.append(os.path.join(input_data, file))\n",
    "        \n",
    "smp_data = [load_smp(file) for file in file_list]\n",
    "\n",
    "print(\"SMP calibration profiles loaded: {}\".format(len(smp_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary information on the snow pit properties\n",
    "\n",
    "**pit_summary** holds summary information on all now pits. Here is a breakdown of the variables within:\n",
    "  - **ID**: Paper site identification code  \n",
    "  - **IOP**: Campaign identity code where a = Alert and e = Eureka  \n",
    "  - **ICE**: Ice surface type where m = MYI and f = FYI  \n",
    "  - **DOY**: Day of year  \n",
    "  - **Date**: Date snow pit was completed  \n",
    "  - **SITEID**: Campaign-specific site identification code  \n",
    "  - **LAT** and **LON** decimal degree WGS-84 coordinates  \n",
    "  - **PITFILE**: Internal ECCC snow pit file names (Available on request)  \n",
    "  - **SMPF**: Last 4 digits of the corresponding SMP file name  \n",
    "  - **MPD** Average of nearby Magnaprobe measured snow thickness  \n",
    "  - **SD**: Total snow pit thickness  \n",
    "  - **PD**: Operator-recorded SMP penetration, better to check directly from SMPF  \n",
    "  - **OFF**: Offset between retracted SMP tip and the snow surface. When mounted on the support structure this generally does not change unless there is settling  \n",
    "  - **NOTESMP**: SMP operator notes  \n",
    "  - **NOTESPIT**: Snow pit operator notes  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pit_summary.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SMP profile total: %i' % len(smp_data))\n",
    "print('Snow pit total: %i' % len(pit_summary))\n",
    "print('\\nDensity total: %i' % len(pit_density))\n",
    "print('\\nSnow pits by campaign:')\n",
    "print(pit_summary.IOP.value_counts())\n",
    "print('\\nSnow pits by ice type:')\n",
    "print(pit_summary.ICE.value_counts())\n",
    "\n",
    "# Note: The small number of recent snow measurements were rolled into rounded classification\n",
    "#       for the paper. In the future many more measurements are needed to chracterize recent snow\n",
    "\n",
    "print('\\nDensity measurements by layer type:')\n",
    "print(pit_density.TYPE.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction of corresponding snow pit density and SMP properties\n",
    "This section relies on functions imported from `smpfunc.py`. We iterate through all the SMP profiles that are associated with snow pits and do the following:\n",
    "\n",
    "1. Restructure the snow pit density estimates into a dataframe (`density_df`)  \n",
    "2. Make first guess estimates of density using the P15 coefficients (`p2015`)  \n",
    "3. Resample the SMP and snow pit arrays to a common thickness axis  \n",
    "4. Generate scaling candidates (`random_tests`) according to `L_RESAMPLE`, `NUM_TESTS`, `MAX_STRETCH_OVERALL`, and `MAX_STRETCH_LAYER` \n",
    "5. Scale the profiles and extract the SMP estimates corresponding with each density cutter measurement (`compare_profiles`)  \n",
    "6. Evaluate retrieval skill according to correlation and rmse (`retrieved_skill`)  \n",
    "7. Save properties of the best scaling candidate for further analysis  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = pd.DataFrame()\n",
    "min_scaling_coeff = []\n",
    "if not skip_alignment:\n",
    "    for smp in smp_data:\n",
    "        smp_file_num = int(smp.name[-4:])\n",
    "        pit_df  = pit_summary[pit_summary['SMPF'] == smp_file_num] # Select the matching pit\n",
    "        density_df = pit_density[pit_density['ID'] == pit_df['ID'].values[0]]\n",
    "        density_df = density_df.assign(relative_height=np.abs(((density_df['TOP']*10) - CUTTER_SIZE) - density_df['TOP'].max()*10).values)\n",
    "\n",
    "        # Make first guess at microstructure based on original profile\n",
    "        l2012 = loewe2012.calc(smp.samples_within_snowpack(), window=WINDOW_SIZE)\n",
    "        p2015 = proksch2015.calc(smp.samples_within_snowpack(), window=WINDOW_SIZE)\n",
    "\n",
    "        # Estimate offset of the snow depth and SMP profile\n",
    "        smp_profile_height = p2015.distance.max()\n",
    "        smp_height_diff = pit_df.MPD.values*1000 - smp_profile_height\n",
    "\n",
    "        # Create new SMP resampled arrays and determine the number of layers\n",
    "        depth_array = np.arange(0, p2015.distance.max() + smp_height_diff, H_RESAMPLE)\n",
    "        density_array = np.interp(depth_array,p2015.distance,p2015.P2015_density)\n",
    "        force_array = np.interp(depth_array,p2015.distance,l2012.force_median)\n",
    "        l_array = np.interp(depth_array,p2015.distance,l2012.L2012_L)\n",
    "\n",
    "        smp_df = pd.DataFrame({'distance': depth_array, \n",
    "                               'density': density_array,\n",
    "                               'force_median': force_array,\n",
    "                               'l': l_array})\n",
    "\n",
    "        # Generate a selection of random transformation to brute-force alignment\n",
    "        # We use this brute force approach because there was no gradient that could be used to optimize the relationship\n",
    "        num_sections = np.ceil(len(smp_df.index)/L_RESAMPLE).astype(int)\n",
    "        random_tests = [smpfunc.random_stretch(x, MAX_STRETCH_OVERALL, MAX_STRETCH_LAYER) for x in np.repeat(num_sections, NUM_TESTS)] \n",
    "\n",
    "        scaled_profiles = [smpfunc.scale_profile(test, smp_df.distance.values, smp_df.density.values, L_RESAMPLE, H_RESAMPLE) for test in random_tests]\n",
    "        compare_profiles = [smpfunc.extract_samples(dist, rho, density_df.relative_height.values, CUTTER_SIZE) for dist, rho in scaled_profiles]\n",
    "        compare_profiles = [pd.concat([profile, density_df.reset_index()], axis=1, sort=False) for profile in compare_profiles]\n",
    "        retrieved_skill = [smpfunc.calc_skill(profile, CUTTER_SIZE) for profile in compare_profiles]\n",
    "        retrieved_skill = pd.DataFrame(retrieved_skill,columns = ['r','rmse','rmse_corr','mae'])\n",
    "\n",
    "        min_scaling_idx = retrieved_skill.sort_values(['r', 'rmse_corr'], ascending=[False, True]).head(1).index.values\n",
    "        min_scaling_coeff.append(random_tests[int(min_scaling_idx)])\n",
    "        \n",
    "        dist, scaled_l =  smpfunc.scale_profile(min_scaling_coeff[-1], smp_df.distance.values, smp_df.l.values, L_RESAMPLE, H_RESAMPLE)\n",
    "        dist, scaled_force_median = smpfunc.scale_profile(min_scaling_coeff[-1], smp_df.distance.values, smp_df.force_median.values, L_RESAMPLE, H_RESAMPLE)\n",
    "\n",
    "        result = compare_profiles[int(min_scaling_idx)].assign(l=smpfunc.extract_samples(dist, scaled_l, density_df.relative_height.values, CUTTER_SIZE).mean_samp,\n",
    "                                                  force_median=smpfunc.extract_samples(dist, scaled_force_median, density_df.relative_height.values, CUTTER_SIZE).mean_samp)\n",
    "        comparison_df = comparison_df.append(result, ignore_index=True)\n",
    "        print(\"Finished with {}\".format(smp.name))\n",
    "    \n",
    "    # Summary of scaling\n",
    "    smp_thickness = [p.samples_within_snowpack().distance.max() for p in smp_data]\n",
    "    scaling_total = [s.sum() for s in min_scaling_coeff]\n",
    "    scaling_mean_abs =  np.round(np.abs(np.array(scaling_total)).mean(), 3)\n",
    "    print('Average scaling: %0.3f' % scaling_mean_abs) # in %\n",
    "\n",
    "    # save the results to a local file since tbe brute-force method takes a while to compute\n",
    "    comparison_df.to_pickle(\"./output/smp_pit_comparison.pkl\")\n",
    "else:\n",
    "    # Load previously-generated results if desired\n",
    "    comparison_df = pd.read_pickle('./output/smp_pit_comparison.pkl')\n",
    "comparison_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering of results\n",
    "Apply a few rules to clean up the data:\n",
    "1. Drop comparisons where any field is not a number (`NaN`), this can occur when density sample overlaps with no SMP data (ie. depth hoar omissions)\n",
    "2. Drop comparisons where the density cutter is not fully within the SMP profile (identified by the SMP count)\n",
    "3. Drop comparisons where there is new snow or ice in the sample (Not enough samples to support robust analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = comparison_df.dropna() \n",
    "result = result[result['count_samp']>=CUTTER_SIZE*2] # Remove comparisons outside the profile\n",
    "result = result[~result['TYPE'].isin(['N', 'I'])] # Remove new snow and ice because we don't have enough samples\n",
    "result['error'] = result['mean_samp']-result['RHO']\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare extracted SMP density against *in situ*\n",
    "SMP density estimates uses the Proksch et. al. 2015 coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P2015 evaluation stats\n",
    "p2015_rmse = np.sqrt(np.mean(result['error']**2))\n",
    "p2015_bias = (result['error']).mean()\n",
    "p2015_r2 = np.ma.corrcoef(result['mean_samp'],result['RHO'])[0, 1]**2\n",
    "p2015_n = len(result['mean_samp'])\n",
    "p2015_p = stats.pearsonr(result['mean_samp'],result['RHO'])[1]\n",
    "\n",
    "print('Proksch et al. 2015 Eval.')\n",
    "print('N: %i' % p2015_n)\n",
    "print('RMSE: %0.1f' % np.round(p2015_rmse))\n",
    "print('bias: %0.1f' % np.round(p2015_bias))\n",
    "print('r^2: %0.2f' % p2015_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paper figure 4 (Top) with caption\n",
    "# 'Evaluation of the SMP density model parametrization of Proksch et al. (2015) (P15; Top) \n",
    "#  and recalibrated coefficients for snow on sea ice (K19b; Bottom). \n",
    "#  In both cases the model is evaluated against manual density cutter measurements of snow density.'\n",
    "\n",
    "axis_value_size = 12\n",
    "axis_label_size = 14\n",
    "\n",
    "line_start = 100\n",
    "line_end = 700\n",
    "point_size = 8\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=False, figsize=(12,5))\n",
    "ax1.tick_params(axis='both', which='major', labelsize=axis_value_size)\n",
    "ax2.tick_params(axis='both', which='major', labelsize=axis_value_size)\n",
    "\n",
    "ax1.scatter(result['RHO'], result['mean_samp'], \n",
    "            s = point_size, color ='black', zorder = 1000)\n",
    "ax1.plot([line_start, line_end], [line_start, line_end], \n",
    "         'k-', color = 'gray' ,alpha= 0.8, zorder = 500)\n",
    "\n",
    "ax1.set_xlim(line_start,line_end)\n",
    "ax1.set_ylim(line_start,line_end)\n",
    "\n",
    "rho_bin_size = 20 #in kg m-3\n",
    "common_bin = np.arange(line_start,line_end, rho_bin_size)\n",
    "hist_kws = dict(density=True, bins=common_bin, histtype=\"stepfilled\", linewidth=1.25)\n",
    "ax2.ticklabel_format(axis='y',style='sci', scilimits=(1,5), useMathText=False)\n",
    "\n",
    "ax2.hist(result['RHO'], alpha = 1, edgecolor=\"black\", \n",
    "         color = 'grey', label = 'Pit', **hist_kws)\n",
    "ax2.hist(result['mean_samp'], alpha = 0.6, edgecolor=\"black\", \n",
    "         color = 'deepskyblue', label = 'SMP', **hist_kws)\n",
    "\n",
    "ax1.text(550, 150,'N: %i \\nRMSE: %i \\nR$^2$: %0.2f'%(p2015_n, p2015_rmse, p2015_r2),  fontsize=12)\n",
    "ax1.set_ylabel('SMP P15 density [kg m$\\mathregular{^{-3}}$]', fontsize=axis_label_size)\n",
    "ax1.set_xlabel('Snow pit density [kg m$\\mathregular{^{-3}}$]', fontsize=axis_label_size)\n",
    "ax2.set_ylabel('PDF', fontsize=axis_label_size)\n",
    "ax2.set_xlabel('Snow density [kg m$\\mathregular{^{-3}}$]', fontsize=axis_label_size)\n",
    "ax2.legend(edgecolor = 'black',  fontsize=axis_value_size)\n",
    "ax2.set_xlim(line_start,line_end);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error as a % of mean density\n",
    "np.round(rmse(result.error)/ result['RHO'].mean(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE by layer type\n",
    "result.groupby('TYPE')['error'].apply(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Errors by campaign (a = alert, e = eureka)\n",
    "campaign_tag = []\n",
    "ice_tag = []\n",
    "lat_tag = []\n",
    "lon_tag = []\n",
    "for x in result['ID']:\n",
    "    ice_tag.append(np.asarray(pit_summary[pit_summary['ID']== x]['ICE'].values))\n",
    "    campaign_tag.append(np.asarray(pit_summary[pit_summary['ID']== x]['IOP'].values))\n",
    "    lat_tag.append(np.asarray(pit_summary[pit_summary['ID']== x]['LAT'].values))\n",
    "    lon_tag.append(np.asarray(pit_summary[pit_summary['ID']== x]['LON'].values))\n",
    "\n",
    "result['campaign'] = np.hstack(campaign_tag)\n",
    "result['ice_type'] = np.hstack(ice_tag)\n",
    "result['lat'] = np.hstack(lat_tag)\n",
    "result['long'] = np.hstack(lon_tag)\n",
    "\n",
    "result.groupby('campaign')['error'].apply(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Errors by ice type (f = fyi, m = myi)\n",
    "result.groupby('ice_type')['error'].apply(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the dataset\n",
    "result.to_pickle(\"./output/smp_pit_filtered.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
