{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated classification of SMP profiles\n",
    "*Josh King, Environment and Climate Change Canada, 2020*\n",
    "\n",
    "\n",
    "In this section we deploy an automated method to classify the SMP transect profiles. The method deploys off-the-shelf tools from [scikit-learn](https://scikit-learn.org) to construct a fairly simple support vector machine (SVM) approach to classification. Others have shown that it is possible to improve classification accuracy with a larger parameter space ([i.e. Havens et al., 2012]( https://ieeexplore.ieee.org/abstract/document/6377289)). We don't do a deep give into big data analyitics or ML here and acknowlesdge it could be improved on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import community packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['text.usetex'] = False\n",
    "import pickle\n",
    "import os\n",
    "from scipy.stats import mode, boxcox\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedShuffleSplit, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC  \n",
    "from sklearn import preprocessing\n",
    "\n",
    "from snowmicropyn import Profile, loewe2012 #SLF python SMP package\n",
    "from smpfunc import preprocess #SMP helper functions\n",
    "\n",
    "# Seed to replicate paper results\n",
    "RANDOM_SEED = 2019\n",
    "\n",
    "# Flag to test reviewer 2 request for ice type in classiciation\n",
    "# Warn: This will not work unless you add the ice type prior to processing\n",
    "use_ice_type = False\n",
    "\n",
    "os.makedirs('./output/sites', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all SMP transect profiles\n",
    "def load_smp(smp_file):\n",
    "    p = Profile.load(smp_file)\n",
    "    p = preprocess(p, smoothing = 0)\n",
    "    ground  = p.detect_ground()\n",
    "    surface  = p.detect_surface()\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from Parts 1 and 2\n",
    "density_coeffs = pickle.load(open('./output/density_k20b_coeffs.pkl', 'rb'))\n",
    "result = pd.read_pickle('./output/smp_pit_filtered.pkl')\n",
    "result = result.assign(force_log=np.log(result['force_median']))\n",
    "result.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create input for the SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the test-train dataset with selected inputs\n",
    "regression_vars = ['relative_height', 'l', 'force_median', 'TYPE']\n",
    "\n",
    "# Reviewer 2 requested that we evaluate ice type as an additional param\n",
    "if use_ice_type:\n",
    "    regression_vars = ['relative_height', 'l', 'force_median', 'TYPE', 'ice_type']\n",
    "\n",
    "svm_dat = result[regression_vars]\n",
    "\n",
    "# Uncomment use ice type as a pred\n",
    "if use_ice_type:\n",
    "    svm_dat['ice_type'].replace('f', 0, inplace= True)\n",
    "    svm_dat['ice_type'].replace('m', 1, inplace= True)\n",
    "\n",
    "\n",
    "y = svm_dat['TYPE']\n",
    "X = svm_dat.drop('TYPE', axis=1)\n",
    "\n",
    "# Uses a stratified shuffle to make sure we get at least a few rounded samples in each train/test split\n",
    "# see https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html\n",
    "stratified_shuffle = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=RANDOM_SEED)\n",
    "stratified_shuffle.get_n_splits(X, y)\n",
    "\n",
    "# Initialize the classifier\n",
    "svclassifier = SVC(kernel='linear', gamma='scale', probability = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy reporting using the 10-fold stratified shuffle\n",
    "scores = cross_val_score(svclassifier, preprocessing.scale(X), y, cv = stratified_shuffle, scoring = 'accuracy')\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviewer 2 asked for a breakdown of the errors by layer type but confusion matrix in scipy does not play well with stratified CV. Its critical that we evaluate accurcy with stratified sampling else accuracy is reported as poor for round types due to under-trainiing rather than a breakdown of theory or physical snow properties. Method is slow and hacky but it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat_full = None \n",
    "for train, test in stratified_shuffle.split(X, y):\n",
    "    svclassifier.fit(X.iloc[train], y.iloc[train])\n",
    "    ypred = svclassifier.predict(X.iloc[test])\n",
    "    conf_mat = confusion_matrix(y.iloc[test].values, ypred, labels=[\"R\", \"F\", \"H\"])\n",
    "    \n",
    "    if conf_mat_full is None:\n",
    "        conf_mat_full = conf_mat\n",
    "    else:\n",
    "        conf_mat_full = conf_mat_full + conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy matrix where rows and cols are R F H\n",
    "print(conf_mat_full/conf_mat_full.sum(axis=1)[:,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on all available data and save\n",
    "svclassifier.fit(X, y)\n",
    "filename = './output/svm_layer_model.pkl'\n",
    "pickle.dump(svclassifier, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function handles classification and stucturing of data type outputs for SMP profiles in the dataset\n",
    "# Input:\n",
    "#   profile: a pandas dataframe structured as data={'relative_height': sn.distance, 'l': sn.L2012_L, 'force_median': sn.force_median}\n",
    "#   classifier: sklearn.svm.classes.SVC \n",
    "\n",
    "def classify_profile(profile, classifier):\n",
    "    temp_profile = profile.copy()\n",
    "    \n",
    "    profile_prediction = classifier.predict(temp_profile)\n",
    "    profile_prediction[profile_prediction=='R'] = 1\n",
    "    profile_prediction[profile_prediction=='F'] = 2\n",
    "    profile_prediction[profile_prediction=='H'] = 3\n",
    "    \n",
    "    temp_profile['layer_type'] = profile_prediction.astype(int)\n",
    "    \n",
    "    #Layers must be at least 1 cm\n",
    "    temp_profile['layer_type'] =  temp_profile['layer_type'].rolling(window=3, min_periods=1).apply(lambda x: mode(x)[0]).values.astype(int)\n",
    "    temp_profile['layer_label'] =  temp_profile['layer_type']\n",
    "    \n",
    "    temp_profile.loc[temp_profile['layer_type'] == 1, 'layer_label'] = 'R'\n",
    "    temp_profile.loc[temp_profile['layer_type'] == 2, 'layer_label'] = 'F'\n",
    "    temp_profile.loc[temp_profile['layer_type'] == 3, 'layer_label'] = 'H'\n",
    "\n",
    "    return temp_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example profile classified\n",
    "Using independent snow pit information to evaluate classification. The selected profile is on MYI near Eureka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smp_profile = './data/SMP/Calibration/S34M0649.pnt'\n",
    "p = Profile.load(smp_profile)\n",
    "p = preprocess(p, 0)\n",
    "ground  = p.detect_ground()\n",
    "surface  = p.detect_surface()\n",
    "profile_height = ground-surface\n",
    "sn = loewe2012.calc(p.samples_within_snowpack(), window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate density\n",
    "log_force_median = np.log(sn.force_median)\n",
    "l = sn.L2012_L\n",
    "density = density_coeffs[0] + density_coeffs[1] * log_force_median + density_coeffs[2] * log_force_median * l + density_coeffs[3] * l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restructure data and classify\n",
    "svm_df = pd.DataFrame(data={'relative_height': sn.distance, 'l': sn.L2012_L, 'force_median': sn.force_median})\n",
    "profile_prediction = classify_profile(svm_df, svclassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the international classification of snow symbols and\n",
    "# set up visuals\n",
    "prop = FontProperties()\n",
    "prop.set_file('./data/SnowSymbolsIACS.ttf')\n",
    "prop.set_size(20)\n",
    "colors = ['#00FF00', '#FFB6C1', '#ADD8E6', '#0000FF']\n",
    "\n",
    "layers = np.abs(np.diff(profile_prediction['layer_type']))\n",
    "transitions = np.insert(np.argwhere(layers>=1).ravel(),0,0,axis = 0)\n",
    "transitions = np.insert(transitions,len(transitions),len(profile_prediction['relative_height'])-2,axis = 0)\n",
    "\n",
    "# Hardcoded layering from the pit sheets\n",
    "obs_interfaces = np.array([61,57,53,44,30.5,13,3])\n",
    "obs_layer_heights = np.abs(obs_interfaces - obs_interfaces.max())*10\n",
    "obs_layer_symbols = ['v','y','z','D','D','F','F']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 6 with caption\n",
    "![png](./output/figures/Fig06_Classification_lowres.png)\n",
    "##### Automated classification on a SMP profile. SMP derived snow density (Black line) is overlaid on the automated classification separated as rounded, faceted, or depth hoar.  Horizontal dashed lines show the heights of snow pit observed stratigraphy at the same location.  Snow pit layer observations are shown as standardized symbols to the right of each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to construct Figure 6\n",
    "axis_value_size = 14\n",
    "axis_label_size = 14\n",
    "\n",
    "f, (ax1, ax2)= plt.subplots(1, 2, sharey=False, figsize=(5,5))\n",
    "ax1.tick_params(axis='both', which='major', labelsize=axis_value_size)\n",
    "ax1.plot(density, profile_prediction.relative_height, color = 'k', zorder=1000, linewidth=2)\n",
    "ax2.axis('off')\n",
    "ax1.set_ylim(0,580)\n",
    "ax1.invert_yaxis()\n",
    "ax1.set_ylabel('Distance below snow surface [mm]',fontsize=axis_label_size)\n",
    "ax1.set_xlabel('Density [kg m$\\mathregular{^{-3}}$]',fontsize=axis_label_size)\n",
    "\n",
    "# Colour in the layers\n",
    "for l in np.arange(0, len(transitions)-1):\n",
    "    layer_class = int(profile_prediction['layer_type'][transitions[l]+1])\n",
    "    polypatch = ax1.axhspan(profile_prediction['relative_height'][transitions[l]], \n",
    "                            profile_prediction['relative_height'][transitions[l+1]], \n",
    "                            alpha=0.5, color=colors[layer_class])\n",
    "\n",
    "# Add in the layer type symbols\n",
    "for l in np.arange(0,len(obs_layer_heights)-1):\n",
    "    layer_height_symbol = (obs_layer_heights[l+1] - (obs_layer_heights[l+1] - obs_layer_heights[l])/2)+10\n",
    "    ax1.text(360, layer_height_symbol, obs_layer_symbols[l], fontproperties=prop)\n",
    "    ax1.axhline(obs_layer_heights[l], color = 'k', alpha =1,lw=2, zorder=500, linestyle='dashed')\n",
    "    \n",
    "legend_elements = [\n",
    "                   Patch(facecolor='#FFB6C1', edgecolor='k',\n",
    "                         label='Rounded'),\n",
    "                   Patch(facecolor='#ADD8E6', edgecolor='k',\n",
    "                         label='Faceted'),\n",
    "                   Patch(facecolor='#0000FF', edgecolor='k',\n",
    "                         label='Depth hoar'),\n",
    "                   Line2D([0], [0], color='k', lw=2, linestyle='dashed', label='Stratigraphy'),\n",
    "                   Line2D([0], [0], color='k', lw=2, label='$\\\\rho_{\\mathrm{smp}}$')]\n",
    "\n",
    "ax1.legend(handles=legend_elements,loc='center', bbox_to_anchor=(1.7, 0.5), ncol=1,  fontsize=axis_value_size)\n",
    "\n",
    "f.savefig('./output/figures/Fig06_Classification_lowres.png', format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify all the SMP transect measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the site directories to be processed\n",
    "site_list = []\n",
    "for dirname, dirnames, filenames in os.walk('.\\data\\SMP\\Sites'):\n",
    "    # print path to all subdirectories first.\n",
    "    for subdirname in dirnames:\n",
    "        if not subdirname.startswith('.'):\n",
    "            site_list.append(os.path.join(dirname, subdirname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process sites from raw SMP profiles to density and aggregate stats\n",
    "expected_classes = ['F', 'R', 'H'] # Some pits don't have all classes so we check\n",
    "\n",
    "def process_site(directory, classifier, coeffs, window_size = 5):\n",
    "    site_measurement_df = pd.DataFrame()\n",
    "    site_summary_df = pd.DataFrame()\n",
    "    file_list = []\n",
    "    site_name = directory.split('\\\\')[-1]\n",
    "    ice_type = directory.split('\\\\')[-1][-3:] \n",
    "\n",
    "    for file in os.listdir(directory):\n",
    "        if file.lower().endswith(\".pnt\"):\n",
    "            file_list.append(os.path.join(directory, file))\n",
    "    print(\"{} has {} SMP files to process... \".format(site_name, len(file_list)), end = '')\n",
    "    \n",
    "    smp_data = [load_smp(file) for file in file_list]\n",
    "    \n",
    "    for p in smp_data:\n",
    "        smp_penetration = p.ground - p.surface\n",
    "        sn = loewe2012.calc(p.samples_within_snowpack(), window=window_size)\n",
    "        sn_df = pd.DataFrame(data={'relative_height': sn.distance, \n",
    "                                   'l': sn.L2012_L, \n",
    "                                   'force_median': sn.force_median})\n",
    "        sn_df =  classify_profile(sn_df, svclassifier)\n",
    "        \n",
    "        sn_df['site_name'] = site_name\n",
    "        sn_df['file_name'] = p.name\n",
    "        sn_df['density'] = coeffs[0] + coeffs[1] * np.log(sn_df['force_median']) \\\n",
    "                         + coeffs[2] * np.log(sn_df['force_median']) * sn_df['l'] \\\n",
    "                         + coeffs[3] * sn_df['l']\n",
    "        \n",
    "        \n",
    "        rho_type_mean = sn_df.groupby(['layer_label'])['density'].mean()\n",
    "        rho_type_std = sn_df.groupby(['layer_label'])['density'].std()\n",
    "        rho_type_fract = sn_df.groupby(['layer_label'])['density'].count()/len(sn_df)\n",
    "        \n",
    "        for l_type in expected_classes:\n",
    "            if not l_type in rho_type_mean.index.values:\n",
    "                rho_type_mean[l_type] = np.nan\n",
    "                rho_type_std[l_type] = np.nan\n",
    "                rho_type_fract[l_type]  = 0\n",
    "       \n",
    "        rho_weighted_mean = (rho_type_mean*rho_type_fract).sum()\n",
    "        \n",
    "        summary_df = pd.DataFrame(data={'file_name': p.name, \n",
    "                                        'site_name': site_name, \n",
    "                                        'ice_type': ice_type,\n",
    "                                        'latitude': p.coordinates[0],\n",
    "                                        'longitude': p.coordinates[1],\n",
    "                                        'penetration': smp_penetration,\n",
    "                                        'density_f': rho_type_mean['F'],\n",
    "                                        'density_h': rho_type_mean['H'],\n",
    "                                        'density_r': rho_type_mean['R'],\n",
    "                                        'fraction_f': rho_type_fract['F'],\n",
    "                                        'fraction_h': rho_type_fract['H'],\n",
    "                                        'fraction_r': rho_type_fract['R'],\n",
    "                                        'mean_weighted_density': rho_weighted_mean},index=[0])\n",
    "        site_summary_df = site_summary_df.append(summary_df, ignore_index=True)\n",
    "        site_measurement_df = site_measurement_df.append(sn_df, ignore_index=True)\n",
    "    \n",
    "   \n",
    "    print(\"Done\")\n",
    "    return site_summary_df, site_measurement_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for site in site_list:\n",
    "    ssum, sdata = process_site(site, svclassifier, density_coeffs)\n",
    "    pickle.dump(ssum, open('./output/sites/Summary_' + ssum.head(1)['site_name'].astype(str)[0] + '.pkl', 'wb'))\n",
    "    pickle.dump(sdata, open('./output/sites/Data_' + ssum.head(1)['site_name'].astype(str)[0]+ '.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
