{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated classification of SMP profiles\n",
    "*Josh King, Environment and Climate Change Canada, 2019*\n",
    "\n",
    "\n",
    "In this section we deploy an automated method to classify the SMP transect profiles. The method applies off-the-shelf tools from [scikit-learn](https://scikit-learn.org) to construct a fairly simple support vector machine. Others have shown that it is possible to improve classification accuracy with a larger parameter space ([i.e. Havens et al., 2013]( https://ieeexplore.ieee.org/abstract/document/6377289)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import community packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['text.usetex'] = False\n",
    "import pickle\n",
    "import os\n",
    "from scipy.stats import mode, boxcox\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedShuffleSplit\n",
    "from sklearn.metrics import classification_report, accuracy_score, make_scorer, r2_score\n",
    "from sklearn.svm import SVC  \n",
    "from sklearn import preprocessing\n",
    "\n",
    "from snowmicropyn import Profile, loewe2012 #SLF python SMP package\n",
    "from smpfunc import preprocess #SMP helper functions\n",
    "\n",
    "# Seed to replicate paper results\n",
    "RANDOM_SEED = 2019\n",
    "\n",
    "os.makedirs('./output/sites', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all SMP transect profiles\n",
    "def load_smp(smp_file):\n",
    "    p = Profile.load(smp_file)\n",
    "    p = preprocess(p, smoothing = 0)\n",
    "    ground  = p.detect_ground()\n",
    "    surface  = p.detect_surface()\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from Parts 1 and 2\n",
    "density_coeffs = pickle.load(open('./output/density_k19b_coeffs.pkl', 'rb'))\n",
    "result = pd.read_pickle('./output/smp_pit_filtered.pkl')\n",
    "result = result.assign(force_log=np.log(result['force_median']))\n",
    "result.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create input for the SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the test-train dataset with selected inputs\n",
    "regression_vars = ['relative_height', 'l', 'force_median', 'TYPE']\n",
    "svm_dat = result[regression_vars]\n",
    "y = svm_dat['TYPE']\n",
    "X = svm_dat.drop('TYPE', axis=1)\n",
    "\n",
    "# Uses a stratified shuffle to make sure we get at least a few rounded samples in each train/test split\n",
    "# see https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html\n",
    "stratified_shuffle = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=RANDOM_SEED)\n",
    "stratified_shuffle.get_n_splits(X, y)\n",
    "\n",
    "# Initialize the classifier\n",
    "svclassifier = SVC(kernel='linear', gamma='scale', probability = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy reporting using the 10-fold stratified shuffle\n",
    "scores = cross_val_score(svclassifier,  preprocessing.scale(X), y, cv = stratified_shuffle)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on all available data and save\n",
    "svclassifier.fit(X, y)\n",
    "filename = './output/svm_layer_model.sav'\n",
    "pickle.dump(svclassifier, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function handles classification and stucturing of data type outputs for SMP profiles in the dataset\n",
    "# Input:\n",
    "#   profile: a pandas dataframe structured as data={'relative_height': sn.distance, 'l': sn.L2012_L, 'force_median': sn.force_median}\n",
    "#   classifier: sklearn.svm.classes.SVC \n",
    "\n",
    "def classify_profile(profile, classifier):\n",
    "    temp_profile = profile.copy()\n",
    "    \n",
    "    profile_prediction = classifier.predict(temp_profile)\n",
    "    profile_prediction[profile_prediction=='R'] = 1\n",
    "    profile_prediction[profile_prediction=='F'] = 2\n",
    "    profile_prediction[profile_prediction=='H'] = 3\n",
    "    \n",
    "    temp_profile['layer_type'] = profile_prediction.astype(int)\n",
    "    \n",
    "    #Layers must be at least 1 cm\n",
    "    temp_profile['layer_type'] =  temp_profile['layer_type'].rolling(window=3, min_periods=1).apply(lambda x: mode(x)[0]).values.astype(int)\n",
    "    temp_profile['layer_label'] =  temp_profile['layer_type']\n",
    "    \n",
    "    temp_profile.loc[temp_profile['layer_type'] == 1, 'layer_label'] = 'R'\n",
    "    temp_profile.loc[temp_profile['layer_type'] == 2, 'layer_label'] = 'F'\n",
    "    temp_profile.loc[temp_profile['layer_type'] == 3, 'layer_label'] = 'H'\n",
    "\n",
    "    return temp_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example profile classified\n",
    "Using independent snow pit information to evaluate classification. The selected profile is on MYI near Eureka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smp_profile = './data/SMP/Calibration/S34M0649.pnt'\n",
    "p = Profile.load(smp_profile)\n",
    "p = preprocess(p, 0)\n",
    "ground  = p.detect_ground()\n",
    "surface  = p.detect_surface()\n",
    "profile_height = ground-surface\n",
    "sn = loewe2012.calc(p.samples_within_snowpack(), window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate density\n",
    "log_force_median = np.log(sn.force_median)\n",
    "l = sn.L2012_L\n",
    "density = density_coeffs[0] + density_coeffs[1] * log_force_median + density_coeffs[2] * log_force_median * l + density_coeffs[3] * l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restructure data and classify\n",
    "svm_df = pd.DataFrame(data={'relative_height': sn.distance, 'l': sn.L2012_L, 'force_median': sn.force_median})\n",
    "profile_prediction = classify_profile(svm_df, svclassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the international classification of snow symbols and\n",
    "# set up visuals\n",
    "prop = FontProperties()\n",
    "prop.set_file('./data/SnowSymbolsIACS.ttf')\n",
    "prop.set_size(20)\n",
    "colors = ['#00FF00', '#FFB6C1', '#ADD8E6', '#0000FF']\n",
    "\n",
    "layers = np.abs(np.diff(profile_prediction['layer_type']))\n",
    "transitions = np.insert(np.argwhere(layers>=1).ravel(),0,0,axis = 0)\n",
    "transitions = np.insert(transitions,len(transitions),len(profile_prediction['relative_height'])-2,axis = 0)\n",
    "\n",
    "# Hardcoded layering from the pit sheets\n",
    "obs_interfaces = np.array([61,57,53,44,30.5,13,3])\n",
    "obs_layer_heights = np.abs(obs_interfaces - obs_interfaces.max())*10\n",
    "obs_layer_symbols = ['v','y','z','D','D','F','F']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paper figure 6  with caption\n",
    "# 'Automated classification on a SMP profile. SMP derived snow density (Black line)\n",
    "#  is overlaid on the automated classification separated as rounded, faceted, or depth hoar. \n",
    "#  Horizontal dashed lines show the heights of snow pit observed stratigraphy at the same location. \n",
    "#  Snow pit layer observations are shown as standardized symbols to the right of each layer.'\n",
    "axis_value_size = 14\n",
    "axis_label_size = 14\n",
    "\n",
    "f, (ax1, ax2)= plt.subplots(1, 2, sharey=False, figsize=(5,5))\n",
    "ax1.tick_params(axis='both', which='major', labelsize=axis_value_size)\n",
    "ax1.plot(density, profile_prediction.relative_height, color = 'k', zorder=1000, linewidth=2)\n",
    "ax2.axis('off')\n",
    "ax1.set_ylim(0,580)\n",
    "ax1.invert_yaxis()\n",
    "ax1.set_ylabel('Distance below snow surface [mm]',fontsize=axis_label_size)\n",
    "ax1.set_xlabel('Density [kg m$\\mathregular{^{-3}}$]',fontsize=axis_label_size)\n",
    "\n",
    "# Colour in the layers\n",
    "for l in np.arange(0, len(transitions)-1):\n",
    "    layer_class = int(profile_prediction['layer_type'][transitions[l]+1])\n",
    "    polypatch = ax1.axhspan(profile_prediction['relative_height'][transitions[l]], \n",
    "                            profile_prediction['relative_height'][transitions[l+1]], \n",
    "                            alpha=0.5, color=colors[layer_class])\n",
    "\n",
    "# Add in the layer type symbols\n",
    "for l in np.arange(0,len(obs_layer_heights)-1):\n",
    "    layer_height_symbol = (obs_layer_heights[l+1] - (obs_layer_heights[l+1] - obs_layer_heights[l])/2)+10\n",
    "    ax1.text(360, layer_height_symbol, obs_layer_symbols[l], fontproperties=prop)\n",
    "    ax1.axhline(obs_layer_heights[l], color = 'k', alpha =1,lw=2, zorder=500, linestyle='dashed')\n",
    "    \n",
    "legend_elements = [\n",
    "                   Patch(facecolor='#FFB6C1', edgecolor='k',\n",
    "                         label='Rounded'),\n",
    "                   Patch(facecolor='#ADD8E6', edgecolor='k',\n",
    "                         label='Faceted'),\n",
    "                   Patch(facecolor='#0000FF', edgecolor='k',\n",
    "                         label='Depth hoar'),\n",
    "                   Line2D([0], [0], color='k', lw=2, linestyle='dashed', label='Stratigraphy'),\n",
    "                   Line2D([0], [0], color='k', lw=2, label='$\\\\rho_{\\mathrm{smp}}$')]\n",
    "\n",
    "ax1.legend(handles=legend_elements,loc='center', bbox_to_anchor=(1.7, 0.5), ncol=1,  fontsize=axis_value_size)\n",
    "\n",
    "f.savefig('./output/figures/Fig06_Classification_lowres.png', format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiy all the smp transect measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the site directories to be processed\n",
    "site_list = []\n",
    "for dirname, dirnames, filenames in os.walk('.\\data\\SMP\\Sites'):\n",
    "    # print path to all subdirectories first.\n",
    "    for subdirname in dirnames:\n",
    "        if not subdirname.startswith('.'):\n",
    "            site_list.append(os.path.join(dirname, subdirname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process sites from raw SMP profiles to density and aggregate stats\n",
    "expected_classes = ['F', 'R', 'H'] # Some pits don't have all classes so we check\n",
    "\n",
    "def process_site(directory, classifier, coeffs, window_size = 5):\n",
    "    site_measurement_df = pd.DataFrame()\n",
    "    site_summary_df = pd.DataFrame()\n",
    "    file_list = []\n",
    "    site_name = directory.split('\\\\')[-1]\n",
    "    ice_type = directory.split('\\\\')[-1][-3:] \n",
    "\n",
    "    for file in os.listdir(directory):\n",
    "        if file.lower().endswith(\".pnt\"):\n",
    "            file_list.append(os.path.join(directory, file))\n",
    "    print(\"{} has {} SMP files to process... \".format(site_name, len(file_list)), end = '')\n",
    "    \n",
    "    smp_data = [load_smp(file) for file in file_list]\n",
    "    \n",
    "    for p in smp_data:\n",
    "        smp_penetration = p.ground - p.surface\n",
    "        sn = loewe2012.calc(p.samples_within_snowpack(), window=window_size)\n",
    "        sn_df = pd.DataFrame(data={'relative_height': sn.distance, \n",
    "                                   'l': sn.L2012_L, \n",
    "                                   'force_median': sn.force_median})\n",
    "        sn_df =  classify_profile(sn_df, svclassifier)\n",
    "        \n",
    "        sn_df['site_name'] = site_name\n",
    "        sn_df['file_name'] = p.name\n",
    "        sn_df['density'] = coeffs[0] + coeffs[1] * np.log(sn_df['force_median']) \\\n",
    "                         + coeffs[2] * np.log(sn_df['force_median']) * sn_df['l'] \\\n",
    "                         + coeffs[3] * sn_df['l']\n",
    "        \n",
    "        \n",
    "        rho_type_mean = sn_df.groupby(['layer_label'])['density'].mean()\n",
    "        rho_type_std = sn_df.groupby(['layer_label'])['density'].std()\n",
    "        rho_type_fract = sn_df.groupby(['layer_label'])['density'].count()/len(sn_df)\n",
    "        \n",
    "        for l_type in expected_classes:\n",
    "            if not l_type in rho_type_mean.index.values:\n",
    "                rho_type_mean[l_type] = np.nan\n",
    "                rho_type_std[l_type] = np.nan\n",
    "                rho_type_fract[l_type]  = 0\n",
    "       \n",
    "        rho_weighted_mean = (rho_type_mean*rho_type_fract).sum()\n",
    "        \n",
    "        summary_df = pd.DataFrame(data={'file_name': p.name, \n",
    "                                        'site_name': site_name, \n",
    "                                        'ice_type': ice_type,\n",
    "                                        'latitude': p.coordinates[0],\n",
    "                                        'longitude': p.coordinates[1],\n",
    "                                        'penetration': smp_penetration,\n",
    "                                        'density_f': rho_type_mean['F'],\n",
    "                                        'density_h': rho_type_mean['H'],\n",
    "                                        'density_r': rho_type_mean['R'],\n",
    "                                        'fraction_f': rho_type_fract['F'],\n",
    "                                        'fraction_h': rho_type_fract['H'],\n",
    "                                        'fraction_r': rho_type_fract['R'],\n",
    "                                        'mean_weighted_density': rho_weighted_mean},index=[0])\n",
    "        site_summary_df = site_summary_df.append(summary_df, ignore_index=True)\n",
    "        site_measurement_df = site_measurement_df.append(sn_df, ignore_index=True)\n",
    "    \n",
    "   \n",
    "    print(\"Done\")\n",
    "    return site_summary_df, site_measurement_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for site in site_list:\n",
    "    ssum, sdata = process_site(site, svclassifier, density_coeffs)\n",
    "    pickle.dump(ssum, open('./output/sites/Summary_' + ssum.head(1)['site_name'].astype(str)[0] + '.sav', 'wb'))\n",
    "    pickle.dump(sdata, open('./output/sites/Data_' + ssum.head(1)['site_name'].astype(str)[0]+ '.sav', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3-smp]",
   "language": "python",
   "name": "conda-env-py3-smp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
